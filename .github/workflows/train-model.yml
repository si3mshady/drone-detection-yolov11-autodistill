name: Drone Detection Training

on:
  push:
    paths:
      - 'images/**'
  workflow_dispatch:

jobs:
  train:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgl1 \
            libglx-mesa0 \
            libglib2.0-0 \
            libsm6 \
            libxext6 \
            libxrender-dev \
            libgomp1 \
            libgdal-dev \
            ffmpeg \
            libsndfile1
      
      - name: Cache Dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install Python Dependencies
        run: |
          pip install --upgrade pip setuptools wheel
          pip install ultralytics autodistill autodistill-grounded-sam
          pip install opencv-python==4.11.0.86 opencv-contrib-python==4.11.0.86
          pip install scipy==1.15.3 scikit-image==0.25.2 scikit-learn==1.6.1
          pip install pillow==11.2.1 imageio==2.37.0
          pip install transformers==4.53.2 datasets==2.14.4
          pip install huggingface-hub==0.33.4 tokenizers==0.21.2
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install numpy==2.0.2 pandas==2.2.2 matplotlib==3.10.0
          pip install requests==2.32.3 urllib3==2.4.0
          pip install tqdm==4.67.1 packaging==25.0
          pip install wandb==0.21.0
          pip install roboflow
      
      - name: Clean Dataset (Clean Slate Method)
        run: |
          echo "üßπ Clean slate - removing old dataset..."
          rm -rf dataset/
      
      - name: Initialize Dataset Structure
        run: |
          echo "üìÅ Creating dataset directory structure..."
          mkdir -p dataset/train/images dataset/train/labels
          mkdir -p dataset/valid/images dataset/valid/labels
          touch dataset/train/images/.gitkeep
          touch dataset/train/labels/.gitkeep
          touch dataset/valid/images/.gitkeep
          touch dataset/valid/labels/.gitkeep
          echo "‚úÖ Dataset structure created"
      
      - name: Count Images
        run: |
          image_count=$(find images/ -name "*.jpg" -o -name "*.jpeg" -o -name "*.png" 2>/dev/null | wc -l)
          echo "üìä Found $image_count images to process"
          if [ $image_count -eq 0 ]; then
            echo "‚ùå No images found in images/ folder"
            exit 1
          fi
          
          # List actual images found
          echo "üìã Images found:"
          find images/ -name "*.jpg" -o -name "*.jpeg" -o -name "*.png" | head -5
      
      - name: Autodistill Labeling and YOLO Training
        env:
          WANDB_API_KEY: "632f1185ffe5698beda8749147a1a6772de21e39"
          PYTHONUNBUFFERED: "1"
          HF_HUB_DISABLE_SYMLINKS_WARNING: "1"
        run: |
          python3 << 'EOF'
          import os
          import glob
          import cv2
          from autodistill_grounded_sam import GroundedSAM
          from autodistill.detection import CaptionOntology
          from ultralytics import YOLO
          
          # Verify images exist and are readable
          print('üîç Verifying images...')
          image_files = glob.glob('images/*.jpg') + glob.glob('images/*.jpeg') + glob.glob('images/*.png')
          print(f'Found {len(image_files)} image files: {image_files}')
          
          for img_path in image_files[:3]:  # Check first 3 images
              try:
                  img = cv2.imread(img_path)
                  if img is not None:
                      print(f'‚úÖ {img_path}: {img.shape}')
                  else:
                      print(f'‚ùå {img_path}: Could not load')
              except Exception as e:
                  print(f'‚ùå {img_path}: Error {e}')
          
          print('üîç Auto-labeling images with GroundedSAM...')
          
          # IMPROVED ONTOLOGY - Broad prompts with lower thresholds
          ontology = CaptionOntology({
              'drone': 'drone',
              'quadcopter': 'drone',
              'aircraft': 'drone'
          })
          
          # Lower thresholds for better detection
          base = GroundedSAM(
              ontology=ontology,
              box_threshold=0.20,   # Lower from default 0.35
              text_threshold=0.20   # Lower from default 0.25
          )
          
          base.label(input_folder='./images', output_folder='dataset')
          
          # Enhanced verification with debugging
          print('üìä Checking dataset creation...')
          train_images = glob.glob('dataset/train/images/*')
          train_labels = glob.glob('dataset/train/labels/*')
          valid_images = glob.glob('dataset/valid/images/*')
          valid_labels = glob.glob('dataset/valid/labels/*')
          
          print(f'üìä Training: {len(train_images)} images, {len(train_labels)} labels')
          print(f'üìä Validation: {len(valid_images)} images, {len(valid_labels)} labels')
          
          # List actual files created
          if train_labels:
              print('‚úÖ Label files created:')
              for label_file in train_labels[:5]:
                  print(f'  üìÑ {label_file}')
                  # Check if label file has content
                  try:
                      with open(label_file, 'r') as f:
                          lines = f.readlines()
                          print(f'      Contains {len(lines)} detection(s)')
                  except:
                      print(f'      Could not read label file')
          else:
              print('‚ùå No label files created')
              print('üîç Debugging: Listing all created files...')
              import subprocess
              result = subprocess.run(['find', 'dataset/', '-type', 'f'], capture_output=True, text=True)
              print('Files in dataset folder:')
              print(result.stdout)
          
          n_labels = len(train_labels)
          n_images = len(train_images)
          print(f'‚úÖ Final count: {n_labels} labels for {n_images} images')
          
          if n_labels == 0:
              raise RuntimeError('‚ùå No labels created - Autodistill did not detect any objects matching the ontology')
          
          # Train YOLO model only if labels exist
          print('üéØ Training YOLOv11 for 50 epochs...')
          model = YOLO('yolo11n-seg.pt')
          model.train(
              data='dataset/data.yaml',
              epochs=50,
              imgsz=640,
              batch=16,
              patience=25,
              verbose=True
          )
          
          print('üéâ Training completed!')
          EOF
      
      - name: Upload Trained Model
        uses: actions/upload-artifact@v4
        if: success()
        with:
          name: drone-model-${{ github.sha }}
          path: |
            runs/segment/train/weights/best.pt
            runs/segment/train/weights/last.pt
          retention-days: 30
      
      - name: Upload Training Logs (Always)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: training-logs-${{ github.sha }}
          path: |
            runs/segment/train/results.csv
            runs/segment/train/results.png
          retention-days: 7
